{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T05:28:27.777020Z",
     "start_time": "2024-12-04T05:28:27.747366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xarray as xr\n",
    "from numpy.linalg import norm\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def boundary_from_contour_with_gradients_2D(xx, yy, Z, boundary_resolution_len: float):\n",
    "    plt.ioff()  # Turn interactive mode off to suppress showing plots\n",
    "\n",
    "    # Extract decision boundary points using contour\n",
    "    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=1, colors=\"black\")\n",
    "    plt.close()\n",
    "\n",
    "    # Get all paths\n",
    "    paths = contours.allsegs[0]  # Get all segments for the contour level\n",
    "\n",
    "    all_interpolated_points = []\n",
    "    all_gradients = []\n",
    "\n",
    "    for path_segment in paths:\n",
    "        vertices = np.array(path_segment)\n",
    "        distances = norm(vertices[1:] - vertices[:-1], axis=1)\n",
    "        cumulative_distances = np.cumsum(distances)\n",
    "        cumulative_distances = np.insert(cumulative_distances, 0, 0)\n",
    "\n",
    "        # Interpolate points along the path\n",
    "        path_length = cumulative_distances[-1]\n",
    "        num_points = int(path_length // boundary_resolution_len) + 1\n",
    "        evenly_spaced_distances = np.linspace(0, path_length, num=num_points)\n",
    "\n",
    "        interp_fn = interp1d(cumulative_distances, vertices, axis=0, kind='linear')\n",
    "        interpolated_points = interp_fn(evenly_spaced_distances)\n",
    "\n",
    "        if len(interpolated_points) < 2:\n",
    "            # Add a dummy gradient (e.g., zero) if only one point\n",
    "            gradients = np.zeros_like(interpolated_points)\n",
    "        else:\n",
    "            # Compute gradient vectors (finite difference method)\n",
    "            gradients = np.zeros_like(interpolated_points)\n",
    "            for i in range(1, len(interpolated_points) - 1):\n",
    "                tangent = interpolated_points[i + 1] - interpolated_points[i - 1]\n",
    "                normal = np.array([-tangent[1], tangent[0]])  # Perpendicular vector\n",
    "                normal /= norm(normal)  # Normalize the gradient\n",
    "                gradients[i] = normal * 0.1  # Scale the gradient (optional)\n",
    "\n",
    "            # Set boundary gradients for the endpoints\n",
    "            gradients[0] = gradients[1]\n",
    "            gradients[-1] = gradients[-2]\n",
    "\n",
    "        # Store points and gradients\n",
    "        all_interpolated_points.append(interpolated_points)\n",
    "        all_gradients.append(gradients)\n",
    "\n",
    "\n",
    "\n",
    "        # Store points and gradients\n",
    "        all_interpolated_points.append(interpolated_points)\n",
    "        all_gradients.append(gradients)\n",
    "\n",
    "    return all_interpolated_points, all_gradients\n",
    "\n",
    "\n",
    "def plot_2D_boundary(X, y, xx, yy, Z, interp_boundary_points, boundary_gradients):\n",
    "    # Plot the decision boundary and data points\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=5, label=\"Data Points\")\n",
    "    plus_minus = 0.1\n",
    "    # plt.contour(xx, yy, Z, levels=[-plus_minus,0,plus_minus], linestyles=['-'], colors='k')\n",
    "    plt.contour(xx, yy, Z, levels=[0,plus_minus], linestyles=['-'], colors='k')\n",
    "\n",
    "    # Plot interpolated points and gradient vectors\n",
    "    # for i, interpolated_points in enumerate(interp_boundary_points):\n",
    "    #     plt.plot(interpolated_points[:, 0], interpolated_points[:, 1], 'go', markersize=5, label=f'Boundary Points (Path {i+1})')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot boundary points and gradient vectors\n",
    "    for i, (interpolated_points, gradients) in enumerate(zip(interp_boundary_points, boundary_gradients)):\n",
    "        plt.plot(interpolated_points[:, 0], interpolated_points[:, 1], 'go', markersize=5)\n",
    "        plt.quiver(\n",
    "            interpolated_points[:, 0], interpolated_points[:, 1],\n",
    "            gradients[:, 0]*.005, gradients[:, 1]*.005,\n",
    "            angles='xy', scale_units='xy', scale=0.01, color='r', alpha=0.8, headwidth=3\n",
    "        )\n",
    "\n",
    "    # Add plot details\n",
    "    plt.title(\"SVM Decision Boundary with Gradient Vectors\")\n",
    "    plt.xlim(X[:, 0].min(), X[:, 0].max())\n",
    "    plt.ylim(X[:, 1].min(), X[:, 1].max())\n",
    "    plt.xlabel(\"x0\")\n",
    "    plt.ylabel(\"x1\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def svm_boundary_from_xarray_2D(dataset:xr.Dataset, param_grid, boundary_resolution_len=0.5, boundary_mesh_resolution = 500):\n",
    "    \"\"\"\n",
    "    Fits an SVM model to the data in the xarray.Dataset and plots the decision boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (xarray.Dataset): The input dataset containing 'points' and 'noise_values'.\n",
    "\n",
    "    Returns:\n",
    "    - None: Plots the SVM decision boundary and data points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the required variables are present\n",
    "    if not {'points', 'noise_values'}.issubset(dataset.variables):\n",
    "        raise ValueError(\"Dataset must contain 'points' and 'noise_values' variables.\")\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = dataset['points'].values  # Shape (num_points, dim)\n",
    "    y = dataset['noise_values'].values  # Shape (num_points,)\n",
    "\n",
    "    # Perform grid search to find the best parameters\n",
    "    svc = SVC(kernel='rbf')\n",
    "    grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best model\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "    # Create a grid for decision boundary visualization\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, boundary_mesh_resolution),\n",
    "                        np.linspace(y_min, y_max, boundary_mesh_resolution))\n",
    "\n",
    "    # Evaluate the decision function\n",
    "    Z = best_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # interp_boundary_points = boundary_from_contour(xx, yy, Z, boundary_resolution_len)\n",
    "\n",
    "\n",
    "    interp_boundary_points, boundary_gradients = boundary_from_contour_with_gradients_2D(xx, yy, Z, boundary_resolution_len)\n",
    "\n",
    "\n",
    "    # Ensure 2D data for visualization\n",
    "    # if X.shape[1] == 2:\n",
    "    #     plot_2D_boundary(X, y, xx, yy, Z, interp_boundary_points, boundary_gradients)\n",
    "\n",
    "    return best_svm, grid_search.best_params_\n"
   ],
   "id": "53aa4c189cbc239b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T05:28:41.471250Z",
     "start_time": "2024-12-04T05:28:27.811831Z"
    }
   },
   "source": [
    "\n",
    "from src.data_generators.perlin_generator import PerlinNoiseGenerator\n",
    "from src.data_samplers.random_sampler import sample_xarray\n",
    "from src.edge_detection.SVM_boundary import SVMBoundary\n",
    "# Initialize the generator with 2D resolution\n",
    "\n",
    "size = 50\n",
    "sample_fracs = [.1, .2, .3, .4, .5, .6, .7]\n",
    "seeds = [1,2,3,4,5,6,7,8,9,10]\n",
    "octs = [1,2, 3,4,5,6,7,8,9,10]\n",
    "each_oct_params = []\n",
    "each_oct_errors = []\n",
    "\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 5, 10, 50, 100, 200, 300],  # Regularization parameter\n",
    "    'gamma': [0.001, 0.1, 0.5, 1, 5, 10, 50, 100]  # Kernel coefficient\n",
    "}\n",
    "\n",
    "for oct in octs:\n",
    "    print(\"oct \", oct)\n",
    "    params = []\n",
    "    errors = []\n",
    "    for sample_frac in sample_fracs:\n",
    "        print(\"sample_frac \", sample_frac)\n",
    "        temp_params = []\n",
    "        temp_errors = []\n",
    "        for seed in seeds:\n",
    "            generator_2D = PerlinNoiseGenerator(res=[size, size], octv=oct, seed=seed, time_test=False)\n",
    "\n",
    "            # Generate Perlin noise data\n",
    "            xdataset_2D = generator_2D.perlin_data_xarray()\n",
    "\n",
    "            svm_boundary, param = svm_boundary_from_xarray_2D(sample_xarray(xdataset_2D, int(sample_frac*size**2)), param_grid, boundary_resolution_len=0.02)\n",
    "            temp_params.append(param)\n",
    "            # generator_2D.plot_2D_xarray()\n",
    "\n",
    "             # Reshape noise values to grid format for visualization\n",
    "            noise_values = xdataset_2D['noise_values'].values.reshape(size, size)  # Reshape to grid\n",
    "            points_x = xdataset_2D['points'].values[:, 0].reshape(size, size)  # x-coordinates reshaped\n",
    "            points_y = xdataset_2D['points'].values[:, 1].reshape(size, size)  # y-coordinates reshaped\n",
    "\n",
    "            X = xdataset_2D['points'].values  # Shape (num_points, dim)\n",
    "            # y = dataset['noise_values'].values  # Shape (num_points,)\n",
    "\n",
    "            # Create a grid of points in the input space\n",
    "            x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "            y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, size), np.linspace(y_min, y_max, size))\n",
    "            grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "            # Predict classifications on the grid\n",
    "            predictions = svm_boundary.predict(grid_points)\n",
    "            predictions = predictions.reshape(xx.shape)\n",
    "            predictions = np.flipud(predictions)  # Flip along both axes\n",
    "            predictions = np.rot90(predictions, k=3)\n",
    "\n",
    "            # # Create a figure with 3 subplots\n",
    "            # fig, axes = plt.subplots(1, 3, figsize=(6, 4))  # 3 rows, 1 column of subplots\n",
    "            #\n",
    "            # # First plot: SVM Classification Regions\n",
    "            # axes[0].contourf(points_x, points_y, predictions, alpha=0.8, cmap='coolwarm')\n",
    "            # axes[0].set_title(\"SVM Classification Regions\")\n",
    "            # axes[0].set_xlabel(\"x0\")\n",
    "            # axes[0].set_ylabel(\"x1\")\n",
    "            #\n",
    "            # # Second plot: True Noise Values\n",
    "            # axes[1].contourf(points_x, points_y, noise_values, alpha=0.8, cmap='coolwarm')\n",
    "            # axes[1].set_title(\"True Noise Values from Perlin Generator\")\n",
    "            # axes[1].set_xlabel(\"x0\")\n",
    "            # axes[1].set_ylabel(\"x1\")\n",
    "\n",
    "            # Compute the difference\n",
    "            difference = noise_values - predictions\n",
    "\n",
    "            # # Third plot: Difference\n",
    "            # axes[2].contourf(points_x, points_y, difference, alpha=0.8, cmap='coolwarm')\n",
    "            # axes[2].set_title(\"Difference Between Dense Dataset and SVM Predictions\")\n",
    "            # axes[2].set_xlabel(\"x0\")\n",
    "            # axes[2].set_ylabel(\"x1\")\n",
    "            #\n",
    "            # # Adjust layout\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "            # Compute absolute differences\n",
    "            abs_diff = np.abs(difference)\n",
    "\n",
    "            # Sum all differences to get the total score\n",
    "            total_score = np.sum(abs_diff)\n",
    "\n",
    "            # Optional: Normalize the score by the number of elements\n",
    "            normalized_score = total_score / difference.size\n",
    "            temp_errors.append(normalized_score)\n",
    "        errors.append(sum(temp_errors)/len(temp_errors))\n",
    "        # params.append(temp_params/temp_params)\n",
    "    each_oct_errors.append(errors)\n",
    "    each_oct_params.append(params)\n",
    "\n",
    "for i in range(len(octs)):\n",
    "    plt.plot(sample_fracs, each_oct_errors[i], label=f\"Oct {octs[i]}\")\n",
    "plt.title(\"Errors per Sample Percent by Octave\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sample Fraction\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(octs, [param['C'] for param in params])\n",
    "# plt.title(\"SVM Parameter C\")\n",
    "# plt.show()\n",
    "# plt.plot(octs, [param['gamma'] for param in params])\n",
    "# plt.title(\"SVM Parameter Gamma\")\n",
    "# plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oct  1\n",
      "sample_frac  0.1\n",
      "Best parameters: {'C': 200, 'gamma': 10}\n",
      "Best parameters: {'C': 5, 'gamma': 50}\n",
      "Best parameters: {'C': 100, 'gamma': 10}\n",
      "Best parameters: {'C': 5, 'gamma': 5}\n",
      "Best parameters: {'C': 10, 'gamma': 10}\n",
      "Best parameters: {'C': 100, 'gamma': 5}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 34\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Generate Perlin noise data\u001B[39;00m\n\u001B[0;32m     32\u001B[0m xdataset_2D \u001B[38;5;241m=\u001B[39m generator_2D\u001B[38;5;241m.\u001B[39mperlin_data_xarray()\n\u001B[1;32m---> 34\u001B[0m svm_boundary, param \u001B[38;5;241m=\u001B[39m svm_boundary_from_xarray_2D(sample_xarray(xdataset_2D, \u001B[38;5;28mint\u001B[39m(sample_frac\u001B[38;5;241m*\u001B[39msize\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)), param_grid, boundary_resolution_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m)\n\u001B[0;32m     35\u001B[0m temp_params\u001B[38;5;241m.\u001B[39mappend(param)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# generator_2D.plot_2D_xarray()\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \n\u001B[0;32m     38\u001B[0m  \u001B[38;5;66;03m# Reshape noise values to grid format for visualization\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[2], line 121\u001B[0m, in \u001B[0;36msvm_boundary_from_xarray_2D\u001B[1;34m(dataset, param_grid, boundary_resolution_len, boundary_mesh_resolution)\u001B[0m\n\u001B[0;32m    119\u001B[0m svc \u001B[38;5;241m=\u001B[39m SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    120\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(svc, param_grid, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 121\u001B[0m grid_search\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Get the best model\u001B[39;00m\n\u001B[0;32m    124\u001B[0m best_svm \u001B[38;5;241m=\u001B[39m grid_search\u001B[38;5;241m.\u001B[39mbest_estimator_\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    964\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    965\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    966\u001B[0m     )\n\u001B[0;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 970\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    974\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1526\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1527\u001B[0m     evaluate_candidates(ParameterGrid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_grid))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    910\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    912\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    914\u001B[0m     )\n\u001B[1;32m--> 916\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    917\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    918\u001B[0m         clone(base_estimator),\n\u001B[0;32m    919\u001B[0m         X,\n\u001B[0;32m    920\u001B[0m         y,\n\u001B[0;32m    921\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    922\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    923\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    924\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[0;32m    925\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[0;32m    926\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[0;32m    927\u001B[0m     )\n\u001B[0;32m    928\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[0;32m    929\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[0;32m    930\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[0;32m    931\u001B[0m     )\n\u001B[0;32m    932\u001B[0m )\n\u001B[0;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    935\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    936\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    937\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    938\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    939\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    127\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    893\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    894\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 895\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    897\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    899\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LibSVM]\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    249\u001B[0m seed \u001B[38;5;241m=\u001B[39m rnd\u001B[38;5;241m.\u001B[39mrandint(np\u001B[38;5;241m.\u001B[39miinfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmax)\n\u001B[1;32m--> 250\u001B[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape_fit_ \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m (n_samples,)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ND_north_star\\Lib\\site-packages\\sklearn\\svm\\_base.py:329\u001B[0m, in \u001B[0;36mBaseLibSVM._dense_fit\u001B[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001B[0m\n\u001B[0;32m    315\u001B[0m libsvm\u001B[38;5;241m.\u001B[39mset_verbosity_wrap(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[0;32m    317\u001B[0m \u001B[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;66;03m# add other parameters to __init__\u001B[39;00m\n\u001B[0;32m    319\u001B[0m (\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_,\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_vectors_,\n\u001B[0;32m    322\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_support,\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdual_coef_,\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_,\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probA,\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probB,\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_status_,\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_iter,\n\u001B[1;32m--> 329\u001B[0m ) \u001B[38;5;241m=\u001B[39m libsvm\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    330\u001B[0m     X,\n\u001B[0;32m    331\u001B[0m     y,\n\u001B[0;32m    332\u001B[0m     svm_type\u001B[38;5;241m=\u001B[39msolver_type,\n\u001B[0;32m    333\u001B[0m     sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m    334\u001B[0m     class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_weight_\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m0\u001B[39m)),\n\u001B[0;32m    335\u001B[0m     kernel\u001B[38;5;241m=\u001B[39mkernel,\n\u001B[0;32m    336\u001B[0m     C\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC,\n\u001B[0;32m    337\u001B[0m     nu\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnu,\n\u001B[0;32m    338\u001B[0m     probability\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprobability,\n\u001B[0;32m    339\u001B[0m     degree\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdegree,\n\u001B[0;32m    340\u001B[0m     shrinking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshrinking,\n\u001B[0;32m    341\u001B[0m     tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtol,\n\u001B[0;32m    342\u001B[0m     cache_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_size,\n\u001B[0;32m    343\u001B[0m     coef0\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoef0,\n\u001B[0;32m    344\u001B[0m     gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gamma,\n\u001B[0;32m    345\u001B[0m     epsilon\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon,\n\u001B[0;32m    346\u001B[0m     max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_iter,\n\u001B[0;32m    347\u001B[0m     random_seed\u001B[38;5;241m=\u001B[39mrandom_seed,\n\u001B[0;32m    348\u001B[0m )\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_from_fit_status()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T05:28:41.487967800Z",
     "start_time": "2024-12-04T04:53:58.101303Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2136c01ccecef35c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86fb7a6bb0a2d083"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
